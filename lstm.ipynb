{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import time\n",
    "import datetime\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from scipy.io import wavfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import signal\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "import keras\n",
    "\n",
    "from keras.layers import Conv2D, BatchNormalization, MaxPooling2D, Dense, Input, Dropout, Flatten, Activation, Conv1D, LSTM\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.initializers import Constant\n",
    "from keras.initializers import he_normal, he_uniform\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "import os\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Sequential\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "from scipy.fftpack import fft\n",
    "import re\n",
    "import gc\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seems to reduce the chance of gpu errors, also doesn't blindly allocate all vram \n",
    "\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_specgram(audio, sample_rate, window_size=20,\n",
    "                 step_size=10, eps=1e-10):\n",
    "    nperseg = int(round(window_size * sample_rate / 1e3))\n",
    "    noverlap = int(round(step_size * sample_rate / 1e3))\n",
    "    freqs, times, spec = signal.spectrogram(audio,\n",
    "                                    fs=sample_rate,\n",
    "                                    window='hann',\n",
    "                                    nperseg=nperseg,\n",
    "                                    noverlap=noverlap,\n",
    "                                    detrend=False)\n",
    "    return freqs, times, np.log(spec.T.astype(np.float32) + eps)\n",
    "\n",
    "def list_wavs_fname(dirpath, ext='wav'):\n",
    "    fpaths = glob(os.path.join(dirpath, r'*/*' + ext))\n",
    "    pat = r'.+\\\\(\\w+)\\\\\\w+\\.wav$'\n",
    "    labels = []\n",
    "    for fpath in fpaths:\n",
    "        r = re.match(pat, fpath)\n",
    "        if r:\n",
    "            labels.append(r.group(1))\n",
    "    pat = r'.+\\\\(\\w+\\.wav)$'\n",
    "    fnames = []\n",
    "    for fpath in fpaths:\n",
    "        r = re.match(pat, fpath)\n",
    "        if r:\n",
    "            fnames.append(r.group(1))\n",
    "    return labels, fnames\n",
    "\n",
    "def pad_audio(samples):\n",
    "    if len(samples) >= L: return samples\n",
    "    else: return np.pad(samples, pad_width=(L - len(samples), 0), mode='constant', constant_values=(0, 0))\n",
    "\n",
    "def chop_audio(samples, L=16000, num=20):\n",
    "    for i in range(num):\n",
    "        beg = np.random.randint(0, len(samples) - L)\n",
    "        yield samples[beg: beg + L]\n",
    "\n",
    "def label_transform(labels):\n",
    "    nlabels = []\n",
    "    for label in labels:\n",
    "        if label == '_background_noise_':\n",
    "            nlabels.append('silence')\n",
    "        elif label not in legal_labels:\n",
    "            nlabels.append('unknown')\n",
    "        else:\n",
    "            nlabels.append(label)\n",
    "    return pd.get_dummies(pd.Series(nlabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 16000\n",
    "legal_labels = 'yes no up down left right on off stop go silence unknown'.split()\n",
    "\n",
    "#src folders\n",
    "root_path = r'.\\\\data'\n",
    "out_path = r'.'\n",
    "model_path = r'.'\n",
    "train_data_path = os.path.join(root_path, 'input', 'train', 'audio')\n",
    "test_data_path = os.path.join(root_path, 'input', 'test', 'audio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\\\\data\\input\\train\\audio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\01124585\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels, fnames = list_wavs_fname(train_data_path)\n",
    "\n",
    "new_sample_rate = 16000\n",
    "y = []\n",
    "x = []\n",
    "\n",
    "for label, fname in zip(labels, fnames):\n",
    "    if label not in legal_labels and label != '_background_noise_' and np.random.randint(10) > 0:\n",
    "        continue\n",
    "    sample_rate, samples = wavfile.read(os.path.join(train_data_path, label, fname))\n",
    "    samples = pad_audio(samples)\n",
    "    if len(samples) > 16000:\n",
    "        n_samples = chop_audio(samples)\n",
    "    else: n_samples = [samples]\n",
    "    for samples in n_samples:\n",
    "        resampled = signal.resample(samples, int(new_sample_rate / sample_rate * samples.shape[0]))\n",
    "        _, _, specgram = log_specgram(resampled, sample_rate=new_sample_rate)\n",
    "        y.append(label)\n",
    "        x.append(specgram)\n",
    "x = np.array(x)\n",
    "y = label_transform(y)\n",
    "label_index = y.columns.values\n",
    "y = y.values\n",
    "y = np.array(y)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "del x, y\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 23, 256)           412416    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 23, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 23, 256)           0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 23, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 23, 128)           197120    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 23, 128)           131584    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 23, 64)            8256      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 23, 64)            0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 23, 64)            0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 23, 12)            780       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 23, 12)            0         \n",
      "=================================================================\n",
      "Total params: 751,180\n",
      "Trainable params: 750,668\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# def cnn_lstm():\n",
    "\n",
    "#     model = Sequential()\n",
    "#     model.add(Conv1D(256, 10, strides=4, input_shape=(99, 161)))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Activation('relu'))\n",
    "#     model.add(Dropout(0.2))\n",
    "        \n",
    "#     model.add(LSTM(128, activation='relu', return_sequences=True, dropout=0.2))\n",
    "#     model.add(LSTM(128, activation='relu', return_sequences=True, dropout=0.2))\n",
    "\n",
    "#     # 1 fully connected layer DNN ReLu with default 20% dropout\n",
    "#     model.add(Dense(64))\n",
    "#     model.add(Activation('relu'))\n",
    "#     model.add(Dropout(0.2))\n",
    "    \n",
    "#     # Output layer with softmax\n",
    "#     model.add(Dense(12))\n",
    "#     model.add(Activation('softmax'))\n",
    "    \n",
    "#     return model\n",
    "\n",
    "# model = cnn_lstm()\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 99, 161)]         0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 23, 256)           412416    \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 23, 256)           1024      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 23, 256)           0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 23, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 23, 128)           197120    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 12)                780       \n",
      "=================================================================\n",
      "Total params: 751,180\n",
      "Trainable params: 750,668\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN, LSTM, Bidirectional, TimeDistributed, Conv1D, ZeroPadding1D, GRU\n",
    "from tensorflow.keras.layers import Lambda, Input, Dropout, Masking, BatchNormalization, Activation\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def cnn_lstm():\n",
    "    input_data = Input(shape=(99, 161))\n",
    "\n",
    "    x = Conv1D(filters=256, kernel_size=10, strides=4)(input_data)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "        \n",
    "    x = LSTM(128, activation='relu', return_sequences=True, dropout=0.2)(x)\n",
    "    x = LSTM(128, activation='relu', return_sequences=False, dropout=0.2)(x)\n",
    "\n",
    "    x = Dense(units=64, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    # Output layer with softmax\n",
    "    y_pred = Dense(units=12, activation='softmax')(x)\n",
    "\n",
    "    network_model = Model(inputs=input_data, outputs=y_pred)\n",
    "    \n",
    "    return network_model\n",
    "\n",
    "K.clear_session()\n",
    "model = cnn_lstm()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22392 samples, validate on 5598 samples\n",
      "Epoch 1/40\n",
      "22392/22392 [==============================] - 10s 467us/sample - loss: 0.2270 - accuracy: 0.9273 - val_loss: 0.2587 - val_accuracy: 0.9198\n",
      "Epoch 2/40\n",
      "22392/22392 [==============================] - 7s 325us/sample - loss: 0.2093 - accuracy: 0.9320 - val_loss: 0.2648 - val_accuracy: 0.9234\n",
      "Epoch 3/40\n",
      "22392/22392 [==============================] - 7s 333us/sample - loss: 0.2163 - accuracy: 0.9306 - val_loss: 0.2454 - val_accuracy: 0.9248\n",
      "Epoch 4/40\n",
      "22392/22392 [==============================] - 7s 335us/sample - loss: 0.2237 - accuracy: 0.9270 - val_loss: 0.2488 - val_accuracy: 0.9237\n",
      "Epoch 5/40\n",
      "22392/22392 [==============================] - 7s 331us/sample - loss: 0.2010 - accuracy: 0.9332 - val_loss: 0.2674 - val_accuracy: 0.9223\n",
      "Epoch 6/40\n",
      "22392/22392 [==============================] - 7s 328us/sample - loss: 0.1953 - accuracy: 0.9361 - val_loss: 0.2571 - val_accuracy: 0.9237\n",
      "Epoch 7/40\n",
      "22392/22392 [==============================] - 7s 333us/sample - loss: 0.1923 - accuracy: 0.9360 - val_loss: 0.2958 - val_accuracy: 0.9198\n",
      "Epoch 8/40\n",
      "22392/22392 [==============================] - 7s 329us/sample - loss: 0.1910 - accuracy: 0.9376 - val_loss: 0.2542 - val_accuracy: 0.9239\n",
      "Epoch 9/40\n",
      "22392/22392 [==============================] - 7s 321us/sample - loss: 0.1878 - accuracy: 0.9400 - val_loss: 0.2405 - val_accuracy: 0.9298\n",
      "Epoch 10/40\n",
      "22392/22392 [==============================] - 7s 321us/sample - loss: 0.1744 - accuracy: 0.9419 - val_loss: 0.2654 - val_accuracy: 0.9287\n",
      "Epoch 11/40\n",
      "22392/22392 [==============================] - 7s 323us/sample - loss: 0.1835 - accuracy: 0.9398 - val_loss: 0.2291 - val_accuracy: 0.9330\n",
      "Epoch 12/40\n",
      "22392/22392 [==============================] - 7s 329us/sample - loss: 0.1729 - accuracy: 0.9445 - val_loss: 0.2767 - val_accuracy: 0.9218\n",
      "Epoch 13/40\n",
      "22392/22392 [==============================] - 7s 323us/sample - loss: 0.1759 - accuracy: 0.9398 - val_loss: 0.2572 - val_accuracy: 0.9323\n",
      "Epoch 14/40\n",
      "22392/22392 [==============================] - 7s 323us/sample - loss: 0.1670 - accuracy: 0.9464 - val_loss: 0.2333 - val_accuracy: 0.9346\n",
      "Epoch 15/40\n",
      "22392/22392 [==============================] - 7s 324us/sample - loss: 0.1591 - accuracy: 0.9486 - val_loss: 0.2414 - val_accuracy: 0.9312\n",
      "Epoch 16/40\n",
      "22392/22392 [==============================] - 7s 323us/sample - loss: 0.1653 - accuracy: 0.9452 - val_loss: 0.2533 - val_accuracy: 0.9319\n",
      "Epoch 17/40\n",
      "22392/22392 [==============================] - 7s 323us/sample - loss: 0.1535 - accuracy: 0.9491 - val_loss: 0.2367 - val_accuracy: 0.9339\n",
      "Epoch 18/40\n",
      "22392/22392 [==============================] - 7s 326us/sample - loss: 0.1456 - accuracy: 0.9509 - val_loss: 0.2403 - val_accuracy: 0.9319\n",
      "Epoch 19/40\n",
      "22392/22392 [==============================] - 7s 327us/sample - loss: 0.1558 - accuracy: 0.9494 - val_loss: 0.2627 - val_accuracy: 0.9296\n",
      "Epoch 20/40\n",
      "22392/22392 [==============================] - 7s 325us/sample - loss: 0.1453 - accuracy: 0.9528 - val_loss: 0.2314 - val_accuracy: 0.9350\n",
      "Epoch 21/40\n",
      "22392/22392 [==============================] - 7s 324us/sample - loss: 0.1552 - accuracy: 0.9481 - val_loss: 0.2461 - val_accuracy: 0.9307\n",
      "Epoch 22/40\n",
      "22392/22392 [==============================] - 7s 323us/sample - loss: 0.1493 - accuracy: 0.9511 - val_loss: 0.2393 - val_accuracy: 0.9335\n",
      "Epoch 23/40\n",
      "22392/22392 [==============================] - 7s 326us/sample - loss: 0.1448 - accuracy: 0.9512 - val_loss: 0.2396 - val_accuracy: 0.9380\n",
      "Epoch 24/40\n",
      "22392/22392 [==============================] - 7s 326us/sample - loss: 0.1442 - accuracy: 0.9527 - val_loss: 0.2291 - val_accuracy: 0.9350\n",
      "Epoch 25/40\n",
      "22392/22392 [==============================] - 7s 324us/sample - loss: 0.1378 - accuracy: 0.9538 - val_loss: 0.2416 - val_accuracy: 0.9353\n",
      "Epoch 26/40\n",
      "22392/22392 [==============================] - 7s 325us/sample - loss: 0.1360 - accuracy: 0.9554 - val_loss: 0.2422 - val_accuracy: 0.9360\n",
      "Epoch 27/40\n",
      "22392/22392 [==============================] - 7s 325us/sample - loss: 0.1359 - accuracy: 0.9559 - val_loss: 0.2345 - val_accuracy: 0.9343\n",
      "Epoch 28/40\n",
      "22392/22392 [==============================] - 7s 325us/sample - loss: 0.1262 - accuracy: 0.9574 - val_loss: 0.2556 - val_accuracy: 0.9341\n",
      "Epoch 29/40\n",
      "22392/22392 [==============================] - 7s 327us/sample - loss: 0.1329 - accuracy: 0.9572 - val_loss: 0.2517 - val_accuracy: 0.9369\n",
      "Epoch 30/40\n",
      "22392/22392 [==============================] - 7s 323us/sample - loss: 0.1348 - accuracy: 0.9560 - val_loss: 0.2773 - val_accuracy: 0.9310\n",
      "Epoch 31/40\n",
      "22392/22392 [==============================] - 7s 325us/sample - loss: 0.1395 - accuracy: 0.9545 - val_loss: 0.2653 - val_accuracy: 0.9293\n",
      "Epoch 32/40\n",
      "22392/22392 [==============================] - 7s 325us/sample - loss: 0.1290 - accuracy: 0.9576 - val_loss: 0.2340 - val_accuracy: 0.9343\n",
      "Epoch 33/40\n",
      "22392/22392 [==============================] - 7s 326us/sample - loss: 0.1412 - accuracy: 0.9552 - val_loss: 0.2775 - val_accuracy: 0.9287\n",
      "Epoch 34/40\n",
      "22392/22392 [==============================] - 7s 326us/sample - loss: 0.1537 - accuracy: 0.9477 - val_loss: 0.2655 - val_accuracy: 0.9310\n",
      "Epoch 35/40\n",
      "22392/22392 [==============================] - 7s 327us/sample - loss: 0.1354 - accuracy: 0.9561 - val_loss: 0.3643 - val_accuracy: 0.9091\n",
      "Epoch 36/40\n",
      "22392/22392 [==============================] - 7s 326us/sample - loss: 0.1820 - accuracy: 0.9400 - val_loss: 0.2510 - val_accuracy: 0.9282\n",
      "Epoch 37/40\n",
      "22392/22392 [==============================] - 7s 324us/sample - loss: 0.1505 - accuracy: 0.9504 - val_loss: 0.2512 - val_accuracy: 0.9332\n",
      "Epoch 38/40\n",
      "22392/22392 [==============================] - 7s 326us/sample - loss: 0.1348 - accuracy: 0.9563 - val_loss: 0.2435 - val_accuracy: 0.9384\n",
      "Epoch 39/40\n",
      "22392/22392 [==============================] - 7s 324us/sample - loss: 0.1221 - accuracy: 0.9600 - val_loss: 0.2454 - val_accuracy: 0.9369\n",
      "Epoch 40/40\n",
      "22392/22392 [==============================] - 7s 333us/sample - loss: 0.1241 - accuracy: 0.9583 - val_loss: 0.2411 - val_accuracy: 0.9353\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "sgd = SGD(lr=0.00001, clipnorm=1.0)\n",
    "adam = Adam(lr=1e-4, clipnorm=1.0)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=128, epochs=40,\n",
    "                    validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'up': 1898,\n",
       "         'off': 1890,\n",
       "         'right': 1906,\n",
       "         'yes': 1901,\n",
       "         'go': 1888,\n",
       "         'down': 1885,\n",
       "         'left': 1885,\n",
       "         'unknown': 3343,\n",
       "         'no': 1909,\n",
       "         'on': 1881,\n",
       "         'stop': 1910,\n",
       "         'silence': 96})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "collections.Counter(label_index[np.argmax(y_train, axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_data_generator(batch):\n",
    "    fpaths = glob(os.path.join(test_data_path, '*wav'))\n",
    "    i = 0\n",
    "    for path in fpaths:\n",
    "        if i == 0:\n",
    "            imgs = []\n",
    "            fnames = []\n",
    "        i += 1\n",
    "        rate, samples = wavfile.read(path)\n",
    "        samples = pad_audio(samples)\n",
    "        resampled = signal.resample(samples, int(new_sample_rate / rate * samples.shape[0]))\n",
    "        _, _, specgram = log_specgram(resampled, sample_rate=new_sample_rate)\n",
    "        imgs.append(specgram)\n",
    "        fnames.append(path.split('\\\\')[-1])\n",
    "        if i == batch:\n",
    "            i = 0\n",
    "            imgs = np.array(imgs)\n",
    "            yield fnames, imgs\n",
    "    if i < batch:\n",
    "        imgs = np.array(imgs)\n",
    "        yield fnames, imgs\n",
    "    raise StopIteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.03% -0.3 seconds left\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "generator raised StopIteration",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-ffebe8cde74f>\u001b[0m in \u001b[0;36mtest_data_generator\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32myield\u001b[0m \u001b[0mfnames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimgs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-70-095451d92729>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mfnames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimgs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_data_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mpredicts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mpredicts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: generator raised StopIteration"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "\n",
    "index = []\n",
    "results = []\n",
    "t0 = datetime.datetime.now()\n",
    "i = 1\n",
    "b = 128\n",
    "\n",
    "for fnames, imgs in test_data_generator(b):\n",
    "    predicts = model.predict(imgs)\n",
    "    predicts = np.argmax(predicts, axis=1)\n",
    "    predicts = [label_index[p] for p in predicts]\n",
    "    index.extend(fnames)\n",
    "    results.extend(predicts)\n",
    "    delta = datetime.datetime.now() - t0\n",
    "    clear_output(wait=True)\n",
    "    print('%.2f%% %.1f seconds left' % (b*i/1585.38, delta.total_seconds()*(158538/b/i-1)))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['fname', 'label'])\n",
    "df['fname'] = index\n",
    "df['label'] = results\n",
    "df.to_csv(os.path.join(out_path, 'submission.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unknown    65841\n",
       "up         11920\n",
       "go         11586\n",
       "silence     8736\n",
       "no          8168\n",
       "right       7890\n",
       "down        7853\n",
       "on          7481\n",
       "off         7456\n",
       "left        7259\n",
       "yes         7196\n",
       "stop        7152\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
